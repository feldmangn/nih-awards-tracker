name: refresh-usaspending

on:
  schedule:
    - cron: "17 2 * * *"
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        working-directory: ./nih-awards-tracker
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # needed for ZIP centroid build:
          pip install pgeocode pandas

      - name: Pull USAspending (last 90 days)
        working-directory: ./nih-awards-tracker
        run: |
          python src/fetch_usaspending.py
          python src/enrich.py data/nih_top_recipients_last_90d.csv

      # NEW: build ZIP -> lat/lon file from the freshly produced CSV
      - name: Build zip_centroids.json
        working-directory: ./nih-awards-tracker
        shell: bash
        run: |
          python - <<'PY'
          import json, pandas as pd
          from pathlib import Path
          import pgeocode

          csv = Path("data/nih_awards_last_90d.csv")
          if not csv.exists():
              raise SystemExit(f"CSV not found: {csv.resolve()}")

          df = pd.read_csv(csv, dtype=str)

          # Be tolerant to header variants
          candidates = [
            "Place Of Performance ZIP Code",
            "Place of Performance ZIP Code",
            "primary place of performance zip code",
            "place of performance zip code",
          ]
          zcol = next((c for c in candidates if c in df.columns), None)
          if not zcol:
            raise SystemExit(f"ZIP column not found. Columns: {df.columns.tolist()}")

          zips = (
              df[zcol]
              .dropna().astype(str)
              .str.extract(r"(\d{5})")[0]
              .dropna().unique().tolist()
          )

          nomi = pgeocode.Nominatim("us")
          out = {}
          for z in zips:
              rec = nomi.query_postal_code(z)
              if pd.notna(rec.latitude) and pd.notna(rec.longitude):
                  out[z] = {"lat": float(rec.latitude), "lon": float(rec.longitude)}

          out_path = Path("data/zip_centroids.json")
          out_path.write_text(json.dumps(out, separators=(",", ":")))
          print(f"Wrote {len(out)} ZIPs -> {out_path}")
          PY

      - name: Publish data for GitHub Pages
        run: |
          mkdir -p docs/data
          echo "Inner data dir:"
          ls -lh nih-awards-tracker/data || true

          # copy ALL csv/json including the new zip_centroids.json
          cp -f nih-awards-tracker/data/*.csv  docs/data/ || true
          cp -f nih-awards-tracker/data/*.json docs/data/ || true

          date -u +"%Y-%m-%d %H:%M UTC" > docs/last_update.txt
          touch docs/.nojekyll

          echo "docs/data after copy:"
          ls -lh docs/data || true

      - name: Update README 'Last Updated' badge text
        run: |
          DATE=$(date -u +"%Y--%m--%d--%H:%M--UTC")
          if ! grep -q "last%20update-" README.md; then
            echo -e "\n![Last Updated](https://img.shields.io/badge/last%20update-$DATE-blue)\n" >> README.md
          else
            sed -i "s|last%20update-[^)]*|last%20update-$DATE|g" README.md
          fi

      - name: Commit updated data, site, and README
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/data/* docs/last_update.txt docs/.nojekyll README.md \
                  nih-awards-tracker/data/*.csv nih-awards-tracker/data/*.json || true
          git commit -m "chore: refresh data & ZIP centroids & timestamps" || echo "No changes"
          git push
