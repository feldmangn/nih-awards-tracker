name: refresh-usaspending

on:
  schedule:
    - cron: "17 2 * * *"   # nightly @ 02:17 UTC
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: refresh-usaspending
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        working-directory: ./nih-awards-tracker
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip install requests pandas pgeocode

      # ---------- Configure which agencies to pull (no heredoc!) ----------
      - name: Prepare agency list
        id: agencies
        working-directory: ./nih-awards-tracker
        env:
          AGLIST: |
            # HHS subtiers
            subtier|National Institutes of Health
            subtier|Advanced Research Projects Agency for Health
            subtier|Agency for Healthcare Research and Quality
            subtier|Centers for Medicare and Medicaid Services
            # DoD (mix of top + subtiers)
            toptier|Department of Defense
            subtier|Office of Naval Research
            subtier|Naval Information Warfare Systems Command
            subtier|Air Force Research Laboratory
            subtier|U.S. Army Medical Research and Development Command
            subtier|U.S. Army Engineer Research and Development Center
            subtier|Defense Health Agency
            # DOE + subtiers
            toptier|Department of Energy
            subtier|Office of Science
            subtier|Advanced Research Projects Agency-Energy
            # EPA
            toptier|Environmental Protection Agency
        run: |
          printf "%s\n" "$AGLIST" > agencies.list
          echo "Wrote agencies.list:" && cat agencies.list

      # ---------- Pull USAspending for ALL agencies in one go ----------
      - name: Pull USAspending (last 90 days, multi-agency)
        working-directory: ./nih-awards-tracker
        env:
          DAYS: "90"
        shell: bash
        run: |
          set -euo pipefail
          ARGS=()
          while IFS='|' read -r TIER NAME; do
            # trim leading/trailing spaces (defensive)
            TIER="$(echo "${TIER:-}" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')"
            NAME="$(echo "${NAME:-}" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')"
            # skip comments/blank/headers
            [[ -z "$TIER" ]] && continue
            [[ "$TIER" == \#* ]] && continue
            if [[ "$TIER" == "toptier" ]]; then
              ARGS+=( --toptier "$NAME" )
            elif [[ "$TIER" == "subtier" ]]; then
              ARGS+=( --subtier "$NAME" )
            fi
          done < agencies.list

          echo "Calling fetch_usaspending.py with flags:"
          printf ' %q' "${ARGS[@]}"; echo
          python src/fetch_usaspending.py --days "$DAYS" "${ARGS[@]}"

      # ---------- (Optional) Enrich top-recipients for EVERY agency ----------
      - name: Enrich top-recipients (all agencies)
        working-directory: ./nih-awards-tracker
        shell: bash
        run: |
          shopt -s nullglob
          for f in data/*_top_recipients_last_90d.csv; do
            echo "Enriching $f"
            python src/enrich.py "$f" || echo "enrich failed for $f (continuing)"
          done

      # ---------- Build ONE zip_centroids.json from ALL agency CSVs ----------
      - name: Build zip_centroids.json (aggregate)
        working-directory: ./nih-awards-tracker
        shell: bash
        run: |
          python - <<'PY'
          import json, glob, pandas as pd
          from pathlib import Path
          import pgeocode

          files = sorted(glob.glob("data/*_awards_last_90d.csv"))
          if not files:
            raise SystemExit("No *_awards_last_90d.csv files found")

          zips = set()
          CANDIDATES = [
              "Place Of Performance ZIP Code",
              "Place of Performance ZIP Code",
              "primary place of performance zip code",
              "place of performance zip code",
          ]
          for f in files:
              try:
                  df = pd.read_csv(f, dtype=str)
              except Exception as e:
                  print(f"Skip {f}: {e}")
                  continue
              zcol = next((c for c in CANDIDATES if c in df.columns), None)
              if not zcol:
                  print(f"{f}: ZIP column not found; columns={list(df.columns)[:8]}...")
                  continue
              zips.update(
                  df[zcol].dropna().astype(str).str.extract(r'(\d{5})')[0].dropna().tolist()
              )

          print(f"Unique ZIPs to geocode: {len(zips)} from {len(files)} CSV(s)")
          nomi = pgeocode.Nominatim('us')
          out = {}
          for z in sorted(zips):
              rec = nomi.query_postal_code(z)
              if pd.notna(rec.latitude) and pd.notna(rec.longitude):
                  out[z] = {"lat": float(rec.latitude), "lon": float(rec.longitude)}

          out_path = Path("data/zip_centroids.json")
          out_path.write_text(json.dumps(out, separators=(",", ":")))
          print(f"Wrote {len(out)} ZIPs -> {out_path}")
          PY

      # ---------- Publish to Pages (copies ALL generated CSV/JSON) ----------
      - name: Publish data for GitHub Pages
        run: |
          set -e
          mkdir -p docs/data
          echo "Inner data dir:"
          ls -lh nih-awards-tracker/data || true
          cp -f nih-awards-tracker/data/*.csv  docs/data/  || true
          cp -f nih-awards-tracker/data/*.json docs/data/  || true
          date -u +"%Y-%m-%d %H:%M UTC" > docs/last_update.txt
          touch docs/.nojekyll
          echo "docs/data after copy:"
          ls -lh docs/data || true

      - name: Update README 'Last Updated' badge text
        run: |
          set -e
          DATE=$(date -u +"%Y--%m--%d--%H:%M--UTC")
          if ! grep -q "last%20update-" README.md; then
            echo -e "\n![Last Updated](https://img.shields.io/badge/last%20update-$DATE-blue)\n" >> README.md
          else
            sed -i "s|last%20update-[^)]*|last%20update-$DATE|g" README.md
          fi

      - name: Commit updated data, site, and README
        run: |
          set -e
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/data/* docs/last_update.txt docs/.nojekyll README.md \
                  nih-awards-tracker/data/*.csv nih-awards-tracker/data/*.json || true
          git commit -m "chore: refresh multi-agency data, centroids & timestamps" || echo "No changes"
          git push
